name: energy-demand-forecasting

x-common-env: &common-env
  env_file:
    - .env

networks:
  obs-net:
    driver: bridge
  plat-net:
    driver: bridge
  airflow-net:
    driver: bridge
  spark-net:
    driver: bridge

volumes:
  # Observability
  grafana_data:
  prometheus_data:
  tempo_data:
  loki_data:

  # Airflow
  airflow_postgres_data:
  airflow_logs:

  # Platform (data + stores)
  minio_data:
  lakefs_data:
  feast_postgres_data:
  forecast_postgres_data:
  mlflow_data:

services:

  # Observability (Grafana stack)
  grafana:
    <<: *common-env
    image: grafana/grafana:latest
    container_name: edf-grafana
    restart: unless-stopped
    networks: [obs-net]
    ports:
      - "${GRAFANA_PORT}:3000"
    environment:
      GF_SECURITY_ADMIN_USER: "${GRAFANA_ADMIN_USER}"
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_ADMIN_PASSWORD}"
      GF_USERS_DEFAULT_THEME: "${GRAFANA_THEME}"
      GF_LOG_LEVEL: "${LOG_LEVEL}"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
      - loki
      - tempo
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3000/api/health | grep -q ok"]
      interval: 10s
      timeout: 5s
      retries: 10

  prometheus:
    <<: *common-env
    image: prom/prometheus:latest
    container_name: edf-prometheus
    restart: unless-stopped
    networks: [obs-net]
    ports:
      - "${PROMETHEUS_PORT}:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yaml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    volumes:
      - prometheus_data:/prometheus
      - ./observability/prometheus.yaml:/etc/prometheus/prometheus.yaml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/ready | grep -q Prometheus"]
      interval: 10s
      timeout: 5s
      retries: 10

  tempo:
    <<: *common-env
    image: grafana/tempo:latest
    container_name: edf-tempo
    restart: unless-stopped
    networks: [obs-net]
    ports:
      - "${TEMPO_PORT}:3200"
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - tempo_data:/var/tempo
      - ./observability/tempo.yaml:/etc/tempo.yaml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3200/ready | grep -q ready"]
      interval: 10s
      timeout: 5s
      retries: 10

  loki:
    <<: *common-env
    image: grafana/loki:latest
    container_name: edf-loki
    restart: unless-stopped
    networks: [obs-net]
    ports:
      - "${LOKI_PORT}:3100"
    command: ["-config.file=/etc/loki.yaml"]
    volumes:
      - loki_data:/loki
      - ./observability/loki.yaml:/etc/loki.yaml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3100/ready | grep -q ready"]
      interval: 10s
      timeout: 5s
      retries: 10

  promtail:
    <<: *common-env
    image: grafana/promtail:latest
    container_name: edf-promtail
    restart: unless-stopped
    networks: [obs-net]
    command: ["-config.file=/etc/promtail.yaml"]
    volumes:
      - ./observability/promtail.yaml:/etc/promtail.yaml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    depends_on:
      - loki

  otel-collector:
    <<: *common-env
    image: otel/opentelemetry-collector-contrib:latest
    container_name: edf-otel-collector
    restart: unless-stopped
    networks: [obs-net]
    ports:
      - "${OTEL_GRPC_PORT}:4317"
      - "${OTEL_HTTP_PORT}:4318"
      - "${OTEL_HEALTH_PORT}:13133"
      - "${OTEL_INTERNAL_METRICS_PORT}:8888"
      - "${OTEL_PROM_EXPORTER_PORT}:8889"
    command: ["--config=/etc/otel-collector.yaml"]
    volumes:
      - ./observability/otel-collector.yaml:/etc/otel-collector.yaml:ro
    depends_on:
      - tempo
      - loki
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:13133/ | grep -q OK"]
      interval: 10s
      timeout: 5s
      retries: 10

  # Data Platform (MinIO + LakeFS)
  minio:
    <<: *common-env
    image: minio/minio:latest
    container_name: edf-minio
    restart: unless-stopped
    networks: [plat-net]
    ports:
      - "${MINIO_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    environment:
      MINIO_ROOT_USER: "${MINIO_ROOT_USER}"
      MINIO_ROOT_PASSWORD: "${MINIO_ROOT_PASSWORD}"
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
  
  minio-init:
    <<: *common-env
    image: minio/mc
    container_name: edf-minio-init
    depends_on:
      - minio
    networks: [plat-net]
    environment:
      MINIO_ROOT_USER: "${MINIO_ROOT_USER}"
      MINIO_ROOT_PASSWORD: "${MINIO_ROOT_PASSWORD}"
      S3_BUCKET: "${S3_BUCKET}"
    entrypoint: >
      sh -c "
        echo '>> Waiting for MinIO...';
        sleep 5;
        mc alias set local http://minio:9000 $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD &&
        mc mb --ignore-existing local/$S3_BUCKET &&
        echo '>> MinIO bucket ensured: '$S3_BUCKET
      "
    restart: "no"

  lakefs-postgres:
    <<: *common-env
    image: postgres:15
    container_name: edf-lakefs-postgres
    restart: unless-stopped
    networks: [plat-net]
    environment:
      POSTGRES_USER: lakefs
      POSTGRES_PASSWORD: lakefs
      POSTGRES_DB: lakefs
    volumes:
      - lakefs_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "lakefs"]
      interval: 5s
      timeout: 5s
      retries: 20

  lakefs:
    <<: *common-env
    image: treeverse/lakefs:latest
    container_name: edf-lakefs
    restart: unless-stopped
    networks: [plat-net]
    ports:
      - "${LAKEFS_PORT}:8000"
    depends_on:
      - lakefs-postgres
      - minio
      - minio-init
    environment:
      LAKEFS_AUTH_ENCRYPT_SECRET_KEY: "${LAKEFS_SECRET_KEY}"
      LAKEFS_DATABASE_TYPE: postgres
      LAKEFS_DATABASE_POSTGRES_CONNECTION_STRING: "postgres://lakefs:lakefs@lakefs-postgres:5432/lakefs?sslmode=disable"
      LAKEFS_BLOCKSTORE_TYPE: s3
      LAKEFS_BLOCKSTORE_S3_ENDPOINT: "${S3_ENDPOINT}"
      LAKEFS_BLOCKSTORE_S3_FORCE_PATH_STYLE: "true"
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_ACCESS_KEY_ID: "${MINIO_ROOT_USER}"
      LAKEFS_BLOCKSTORE_S3_CREDENTIALS_SECRET_ACCESS_KEY: "${MINIO_ROOT_PASSWORD}"
      LAKEFS_BLOCKSTORE_S3_REGION: "${S3_REGION}"

  lakefs-bootstrap:
    <<: *common-env
    image: curlimages/curl:latest
    container_name: edf-lakefs-bootstrap
    depends_on:
      - lakefs
      - minio-init
    networks: [plat-net]
    volumes:
      - ../../infra/lakefs/init/init_repo.sh:/init/init_repo.sh:ro
    entrypoint: ["sh", "/init/init_repo.sh"]
    restart: "no"

  # Stores: Feast offline + Forecast store
  feast-postgres:
    <<: *common-env
    image: postgres:15
    container_name: edf-feast-postgres
    restart: unless-stopped
    networks: [plat-net]
    environment:
      POSTGRES_USER: "${FEAST_PG_USER}"
      POSTGRES_PASSWORD: "${FEAST_PG_PASSWORD}"
      POSTGRES_DB: "${FEAST_PG_DB}"
    ports:
      - "${FEAST_PG_PORT}:5432"
    volumes:
      - feast_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "feast"]
      interval: 5s
      timeout: 5s
      retries: 20

  forecast-postgres:
    <<: *common-env
    image: postgres:15
    container_name: edf-forecast-postgres
    restart: unless-stopped
    networks: [plat-net]
    environment:
      POSTGRES_USER: "${FORECAST_PG_USER}"
      POSTGRES_PASSWORD: "${FORECAST_PG_PASSWORD}"
      POSTGRES_DB: "${FORECAST_PG_DB}"
    ports:
      - "${FORECAST_PG_PORT}:5432"
    volumes:
      - forecast_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "forecast"]
      interval: 5s
      timeout: 5s
      retries: 20

  # Online store
  redis:
    <<: *common-env
    image: redis:7
    container_name: edf-redis
    restart: unless-stopped
    networks: [plat-net]
    ports:
      - "${REDIS_PORT}:6379"

  # MLflow (tracking + registry)
  mlflow:
    <<: *common-env
    image: ghcr.io/mlflow/mlflow:latest
    container_name: edf-mlflow
    restart: unless-stopped
    networks: [plat-net, obs-net]
    ports:
      - "${MLFLOW_PORT}:5000"
    environment:
      MLFLOW_TRACKING_URI: "${MLFLOW_TRACKING_URI}"
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
    volumes:
      - mlflow_data:/mlflow

  # Spark (for feature + transform jobs)
  spark-master:
    <<: *common-env
    platform: linux/amd64
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    restart: unless-stopped
    networks: [spark-net, plat-net]
    ports:
      - "${SPARK_MASTER_UI_PORT:-8081}:8080"
      - "${SPARK_MASTER_PORT:-7077}:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark

  spark-worker:
    <<: *common-env
    platform: linux/amd64
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker
    restart: unless-stopped
    networks: [spark-net, plat-net]
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077

  # Airflow (LocalExecutor for local dev)
  airflow-postgres:
    <<: *common-env
    image: postgres:15
    container_name: edf-airflow-postgres
    restart: unless-stopped
    networks: [airflow-net]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "${AIRFLOW_PG_PORT}:5432"
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      timeout: 5s
      retries: 20

  airflow-init:
    <<: *common-env
    build:
      context: ../..
      dockerfile: infra/airflow/Dockerfile
    container_name: edf-airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    networks: [airflow-net, plat-net, spark-net, obs-net]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
      AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT: "60"
      AIRFLOW__WEBSERVER__SECRET_KEY: "dev-secret-key"
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "${AIRFLOW__CORE__DEFAULT_TIMEZONE}"
      PYTHONPATH: /opt/edf
      MLFLOW_TRACKING_URI: "${MLFLOW_TRACKING_URI_INTERNAL}"
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4318"
      JAVA_HOME: "/usr/lib/jvm/java-17-openjdk-arm64"
    volumes:
      - ../../:/opt/edf
      - airflow_logs:/opt/airflow/logs
      - ../../infra/airflow/dags:/opt/airflow/dags
    command:
      - bash
      - -c
      - airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true

  airflow-webserver:
    <<: *common-env
    build:
      context: ../..
      dockerfile: infra/airflow/Dockerfile
    container_name: edf-airflow-webserver
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks: [airflow-net, plat-net, spark-net, obs-net]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: "dev-secret-key"
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "${AIRFLOW__CORE__DEFAULT_TIMEZONE}"
      PYTHONPATH: /opt/edf
      MLFLOW_TRACKING_URI: "${MLFLOW_TRACKING_URI_INTERNAL}"
      JAVA_HOME: "/usr/lib/jvm/java-17-openjdk-arm64"
    volumes:
      - ../../:/opt/edf
      - airflow_logs:/opt/airflow/logs
      - ../../infra/airflow/dags:/opt/airflow/dags
    ports:
      - "${AIRFLOW_PORT}:8080"
    command: webserver

  airflow-scheduler:
    <<: *common-env
    build:
      context: ../..
      dockerfile: infra/airflow/Dockerfile
    container_name: edf-airflow-scheduler
    restart: unless-stopped
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    networks: [airflow-net, plat-net, spark-net, obs-net]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DEFAULT_TIMEZONE: "${AIRFLOW__CORE__DEFAULT_TIMEZONE}"
      AIRFLOW__WEBSERVER__SECRET_KEY: "dev-secret-key"
      AIRFLOW__CORE__FERNET_KEY: ${FERNET_KEY}
      PYTHONPATH: /opt/edf
      MLFLOW_TRACKING_URI: "${MLFLOW_TRACKING_URI_INTERNAL}"
      JAVA_HOME: "/usr/lib/jvm/java-17-openjdk-arm64"
    volumes:
      - ../../:/opt/edf
      - airflow_logs:/opt/airflow/logs
      - ../../infra/airflow/dags:/opt/airflow/dags
    command: scheduler
