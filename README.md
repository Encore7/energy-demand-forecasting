# âš¡ Real-Time Energy Demand Forecasting System

A production-ready, **local-first** machine learning system for forecasting **Germany's energy demand** in:
- **Day-Ahead (DA)** market (24Ã—1h forecast, once daily)
- **Intraday (ID)** market (rolling forecasts every 5â€“15 minutes)

Designed to run locally via **Docker Compose** for staging, but fully portable to **Kubernetes** or cloud environments.

---

## Architecture Overview

The system has two independent ML pipelines:

### 1. Day-Ahead (DA) â€” Batch (PySpark)
- **Cadence:** Once per day at cutoff time
- **Features:** Slow-changing (yesterdayâ€™s load, tomorrowâ€™s weather forecast, calendar)
- **Processing:** PySpark batch jobs (Bronze â†’ Silver â†’ Gold DA)
- **Feature Store:** Feast **Offline** (Parquet in MinIO/lakeFS)
- **Serving:** Precomputes next dayâ€™s forecast â†’ stores in Parquet + Kafka â†’ served via `da-svc` (FastAPI) instantly
- **Model:** `model_da` in MLflow Registry

### 2. Intraday (ID) â€” Streaming (PyFlink)
- **Cadence:** Continuous; updates every 5â€“15 min
- **Features:** Fast-changing (current load, latest weather nowcasts, updated market prices)
- **Processing:** PyFlink streaming jobs â†’ real-time feature calculation
- **Feature Store:** Feast **Online** (Redis)
- **Serving:** `id-svc` (FastAPI) pulls features from Redis â†’ low-latency scoring in memory
- **Model:** `model_id` in MLflow Registry

---

## System Design

<p align="center">
  <img src="system-design.svg" alt="System Design Diagram" width="100%">
</p>

---

## Data Flow (high level)

- **DA Path:**  
  Bronze â†’ PySpark (Silver) â†’ PySpark (Gold DA) â†’ Feast Offline â†’ MLflow model â†’ Batch inference â†’ Stored forecast â†’ API + Grafana
- **ID Path:**  
  Kafka â†’ PyFlink â†’ Feast Online (Redis) â†’ FastAPI inference â†’ Forecast â†’ Grafana

---

## ğŸ› ï¸ Tech Stack

| Layer                | Tooling |
|----------------------|---------|
| **Messaging**        | Kafka, Karapace Schema Registry |
| **Batch Processing** | PySpark |
| **Stream Processing**| PyFlink |
| **Storage**          | MinIO (S3 API), lakeFS (versioning), Parquet |
| **Feature Store**    | Feast (Offline = Parquet, Online = Redis) |
| **Model Registry**   | MLflow Tracking + Registry |
| **Orchestration**    | Airflow |
| **Validation**       | Great Expectations |
| **Drift Monitoring** | Evidently |
| **Serving**          | FastAPI (`da-svc`, `id-svc`) |
| **Observability**    | OpenTelemetry â†’ Prometheus (metrics), Loki (logs), Tempo (traces), Grafana (dashboards + alerts) |

---

## Pipelines

### Day-Ahead (DA)
1. **Ingestion:** Data arrives in Kafka â†’ Lakehouse (Bronze)
2. **Batch Prep:** PySpark â†’ Silver â†’ Gold DA
3. **Validation:** Great Expectations on Silver/Gold
4. **Feature Store:** Write Gold features to Feast Offline
5. **Training:** Run daily/weekly; log to MLflow
6. **Inference:** Load `model_da` from MLflow â†’ predict â†’ store forecast in Parquet + Kafka
7. **Serving:** `da-svc` returns precomputed forecast

### Intraday (ID)
1. **Ingestion:** Data arrives in Kafka
2. **Stream Processing:** PyFlink calculates real-time features â†’ writes to Redis (Feast Online)
3. **Serving:** `id-svc` pulls features from Redis â†’ runs `model_id` in memory â†’ returns predictions
4. **(Optional)** Store ID forecasts in Kafka/Parquet for audit and dashboarding

---

## Models

- **`model_da`** â€” Day-Ahead  
  Trained on Gold DA features from Feast Offline

- **`model_id`** â€” Intraday  
  Trained on historical Gold ID features backfilled from PyFlink outputs

Both are versioned and promoted in **MLflow Registry** with guardrails.

---

## Observability

- **Metrics:** Prometheus (pipeline durations, API latency, drift metrics)
- **Logs:** Loki
- **Traces:** Tempo (end-to-end spans from ingestion â†’ serving)
- **Dashboards & Alerts:** Grafana